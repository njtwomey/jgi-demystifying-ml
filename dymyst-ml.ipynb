{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demystifying Machine Learning Demo Session\n",
    "\n",
    "Peter Flach and Niall Twomey\n",
    "\n",
    "Tuesday, 5th of December 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Machine Learning Pipeline\n",
    "\n",
    "Need to prepare data into a matrix of observations X and a vector of labels y. \n",
    "\n",
    "![Machine Learning Pipeline](ml-pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CASAS Dataset\n",
    "\n",
    "This notebook considers the CASAS dataset. This is a dataset collected in a smart environment. As participants interact with the house, sensors record their interactions. There are a number of different sensor types including motion, door contact, light, temperature, water flow, etc.\n",
    "\n",
    "This notebook goes through a number of common issues in data science and machine learning pipelines when working with real data. Namely, several issues relating to dates, sensor values, etc. This are dealt with consistently using the functionality provided by the pandas library.\n",
    "\n",
    "The objective is to fix all errors (if we can), and then to convert the timeseries data to a form that would be recognisable by a machine learning algorithm. I have attempted to comment my code where possible to explain my thought processes. At several points in this script I could have taken shortcuts, but I also attempted to forgo brevity for clarity.\n",
    "\n",
    "Resources: \n",
    "- CASAS homepage: http://casas.wsu.edu\n",
    "- Pandas library: https://pandas.pydata.org/\n",
    "- SKLearn library: http://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CASAS Testbed](sensorlayout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the libraries that we need to use \n",
    "\n",
    "from os.path import join \n",
    "\n",
    "import matplotlib.pyplot as pl \n",
    "import seaborn as sns \n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "% matplotlib inline \n",
    "\n",
    "sns.set_style('darkgrid') \n",
    "sns.set_context('poster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data \n",
    "\n",
    "url = 'http://casas.wsu.edu/datasets/twor.2009.zip'\n",
    "\n",
    "zipfile = url.split('/')[-1]\n",
    "dirname = '.'.join(zipfile.split('.')[:2])\n",
    "filename = join(dirname, 'data')\n",
    "\n",
    "print('     url: {}'.format(url))\n",
    "print(' zipfile: {}'.format(zipfile))\n",
    "print(' dirname: {}'.format(dirname))\n",
    "print('filename: {}'.format(filename))\n",
    "\n",
    "call(('wget', url))\n",
    "call(('unzip', zipfile))\n",
    "call(('rm', 'twor.2009.zip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data file\n",
    "\n",
    "column_headings = ('date', 'time', 'sensor', 'value', 'annotation', 'state')\n",
    "\n",
    "df = pd.read_csv(\n",
    "    filename, \n",
    "    delim_whitespace=True, \n",
    "    names=column_headings\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in column_headings:\n",
    "    df.loc[:, col] = df[col].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small diversion: pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sensor.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.annotation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.state.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not everything is what it seems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The date and time columns are generic python **objects**. We will want them to be date time objects so that we can work with them naturally. Before so doing we will want to verify that all of the data are proper dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final date is clearly incorrect. We can assume that '22009' is intended to be '2009'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df.date.str.startswith('22009'), 'date'] = '2009-02-03'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the date time objects and set them as the index of the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['datetime'] = pd.to_datetime(df[['date', 'time']].apply(lambda row: ' '.join(row), axis=1))\n",
    "\n",
    "df = df[['datetime', 'sensor', 'value', 'annotation', 'state']]\n",
    "df.set_index('datetime', inplace=True)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index.second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying the sensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CASAS Testbed](sensorlayout.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sensor.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- M-sensors are binary motion sensors (ON/OFF)\n",
    "- L-sensors are ambiant light sensors (ON/OFF)\n",
    "- D-sensors are binary door sensors (OPEN/CLOSED)\n",
    "- I-sensors are binary item presence sensors (PRESENT/ABSENT)\n",
    "- A-sensors are ADC (measuring temperature on hob/oven)\n",
    "\n",
    "M-, L-, I- and D-sensors are binary, whereas A-sensors have continuous values. So let's split them up into analogue and digital dataframes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the analogue and digital components from eachother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf = df[~df.sensor.str.startswith(\"A\")][['sensor', 'value']]\n",
    "adf = df[df.sensor.str.startswith(\"A\")][['sensor', 'value']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorical data\n",
    "\n",
    "We would like to create a matrix columns corresponding to the categorical sensor name (eg M13) which is `1` when the sensor value is `ON`, `-1` when the sensor value is `OFF`, and otherwise remains `0`. First we need to validate the values of the categorical dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf.value.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some strange values: \n",
    "\n",
    "- ONF\n",
    "- OF\n",
    "- O\n",
    "- OFFF\n",
    "\n",
    "It is often unclear how we should deal with errors such as these, so let's just convert the sensor value of all of these to `ON` in this demo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in ('ONF', 'OF', 'O', 'OFFF'): \n",
    "    cdf.loc[cdf.value == value, 'value'] = 'ON'\n",
    "cdf.value.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_cols = pd.get_dummies(cdf.sensor)\n",
    "cdf_cols.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cdf_cols['M35'].plot(figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "kitchen_columns = ['M{}'.format(ii) for ii in (15, 16, 17, 18, 19, 51)]\n",
    "\n",
    "start = datetime(2009, 2, 2, 10)\n",
    "end   = datetime(2009, 2, 2, 11)\n",
    "cdf_cols[(cdf_cols.index > start) & (cdf_cols.index < end)][kitchen_columns].plot(subplots=True, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime(2009, 2, 2, 15)\n",
    "end   = datetime(2009, 2, 2, 17)\n",
    "cdf_cols[(cdf_cols.index > start) & (cdf_cols.index < end)][kitchen_columns].plot(subplots=True, figsize=(10, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analogue data\n",
    "\n",
    "the `value` column of the `adf` dataframe is still a set of strings, so let's convert these to floating point numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.value.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_inds = adf.value.str.endswith('F') \n",
    "adf.loc[f_inds, 'value'] = adf.loc[f_inds, 'value'].str[:-1]\n",
    "\n",
    "f_inds = adf.value.str.startswith('F') \n",
    "adf.loc[f_inds, 'value'] = adf.loc[f_inds, 'value'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.loc[:, 'value'] = adf.value.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.value.groupby(adf.sensor).plot(kind='kde', legend=True, figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_keys = adf.sensor.unique()\n",
    "adf_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_cols = pd.get_dummies(adf.sensor)\n",
    "for key in adf_keys:\n",
    "    adf_cols[key] *= adf.value\n",
    "\n",
    "adf_cols = adf_cols[adf_keys]\n",
    "adf_cols.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrouping \n",
    "\n",
    "At this stage we have our data prepared as we need. We have arranged the categorical data into a matrix of 0 and 1, and the analogue data has also been similarly translated. What remains is to produce our label matrix. Since we have already introduced most of the methods in the previous sections, this should be quite straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_inds = pd.notnull(df.annotation)\n",
    "\n",
    "anns = df.loc[annotation_inds][['annotation', 'state']]\n",
    "\n",
    "# Removing duplicated indices\n",
    "anns = anns.groupby(level=0).first()\n",
    "anns.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly there are also bugs in the labels! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for annotation, group in anns.groupby('annotation'): \n",
    "    counts = group.state.value_counts()\n",
    "    \n",
    "    if counts.begin == counts.end: \n",
    "        print('             {}: equal counts ({} begins, {} ends)'.format(\n",
    "            annotation, \n",
    "            counts.begin, \n",
    "            counts.end\n",
    "        ))\n",
    "        \n",
    "    else:\n",
    "        print(' *** WARNING {}: inconsistent annotation counts with {} begins and {} ends'.format(\n",
    "            annotation, \n",
    "            counts.begin, \n",
    "            counts.end\n",
    "        ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def filter_annotations(anns):\n",
    "    left = iter(anns.index[:-1])\n",
    "    right = iter(anns.index[1:])\n",
    "\n",
    "    filtered_annotations = []\n",
    "    for ii, (ll, rr) in enumerate(zip(left, right)): \n",
    "        l = anns.loc[ll]\n",
    "        r = anns.loc[rr]\n",
    "\n",
    "        if l.state == 'begin' and r.state == 'end': \n",
    "            filtered_annotations.append(dict(label=l.annotation, start=ll, end=rr))\n",
    "                \n",
    "    return filtered_annotations\n",
    "        \n",
    "\n",
    "annotations = []\n",
    "for annotation, group in anns.groupby('annotation'): \n",
    "    gi = filter_annotations(group)\n",
    "    if len(gi) > 10:\n",
    "        print('{:>30} - {}'.format(annotation, len(group)))\n",
    "        annotations.extend(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_a = []\n",
    "X_d = []\n",
    "y   = []\n",
    "\n",
    "for ann in annotations: \n",
    "    try: \n",
    "        ai = adf_cols[ann['start']: ann['end']]\n",
    "        ci = cdf_cols[ann['start']: ann['end']]\n",
    "        yi = ann['label']\n",
    "\n",
    "        X_a.append(ai)\n",
    "        X_d.append(ci)\n",
    "        y.append(yi)\n",
    "        \n",
    "    except KeyError: \n",
    "        pass\n",
    "\n",
    "print(len(y), len(X_d), len(X_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ii = 10\n",
    "print(y[ii])\n",
    "print(X_d[ii].sum().to_dict())\n",
    "print(X_a[ii].sum().to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "\n",
    "for ii in range(len(y)):\n",
    "    xi = dict()\n",
    "    \n",
    "    # Number of sensor activations\n",
    "    xi['nd'] = len(X_d)\n",
    "    xi['na'] = len(X_a)\n",
    "    \n",
    "    # Duration of sensor windows\n",
    "    if len(X_d[ii]): \n",
    "        xi['dd'] = (X_d[ii].index[-1] - X_d[ii].index[0]).total_seconds()\n",
    "    if len(X_a[ii]):\n",
    "        xi['da'] = (X_a[ii].index[-1] - X_a[ii].index[0]).total_seconds()\n",
    "    \n",
    "    for xx in (X_a[ii], X_d[ii]): \n",
    "        # Value counts of sensors \n",
    "        for kk, vv in xx.sum().to_dict().items(): \n",
    "            if np.isfinite(vv) and vv > 0:\n",
    "                xi[kk] = vv\n",
    "                                \n",
    "        # Average time of day\n",
    "        for kk, vv in xx.index.hour.value_counts().to_dict().items(): \n",
    "            kk = 'H_{}'.format(kk)\n",
    "            if kk not in xi: \n",
    "                xi[kk] = 0\n",
    "            xi[kk] += vv\n",
    "    X.append(xi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(10): \n",
    "    print(y[ii], X[ii], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing machine learning on this (FINALLY!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Cross validation\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "model_classes = [\n",
    "    LogisticRegression, \n",
    "    RandomForestClassifier, \n",
    "    SVC, \n",
    "    GaussianNB, \n",
    "    KNeighborsClassifier, \n",
    "    DecisionTreeClassifier, \n",
    "]\n",
    "\n",
    "print('Learning models...', end='')\n",
    "for model_class in model_classes:\n",
    "    folds = StratifiedKFold(5, shuffle=True, random_state=12345)\n",
    "    for fold_i, (train_inds, test_inds) in enumerate(folds.split(X, y)): \n",
    "        print('.', end='')\n",
    "        X_train, y_train = [X[i] for i in train_inds], [y[i] for i in train_inds]\n",
    "        X_test, y_test = [X[i] for i in test_inds], [y[i] for i in test_inds]\n",
    "\n",
    "        model = Pipeline((\n",
    "            ('dict_to_vec', DictVectorizer(sparse=False)), \n",
    "            ('scaling', StandardScaler()), \n",
    "            ('classifier', model_class()),\n",
    "        ))\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        results.append(dict(\n",
    "            model=model_class.__name__, \n",
    "            fold=fold_i, \n",
    "            train_acc=model.score(X_train, y_train),\n",
    "            test_acc=model.score(X_test, y_test)\n",
    "        ))\n",
    "print('...done!\\n')\n",
    "                \n",
    "res = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.groupby('model')[['train_acc', 'test_acc']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflecting on Pipeline\n",
    "\n",
    "![Machine Learning Pipeline](ml-pipeline.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take away messages\n",
    "\n",
    "- Be cynical about data! \n",
    "- Ensuring that data is in an appropriate form is very important. \n",
    "- Discovering the variety of errors in the data is not easy, depends on the application, and can be present even in fully automated systems. \n",
    "- Using machine learning models on data doesn't have to take too much time, although developing bespoke model classes for specific applications will. \n",
    "- Part of modelling the problem is working with noisy data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
